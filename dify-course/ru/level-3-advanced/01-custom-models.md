---
title: "Кастомные модели"
weight: 1
bookToc: true
---

# Кастомные модели

## Зачем?

Вам могут понадобиться модели, которых нет в Dify по умолчанию — локальные, дообученные или на приватных серверах.

## Поддерживаемые типы

| Тип | Примеры |
|-----|---------|
| **OpenAI-совместимые** | vLLM, LocalAI, Together AI |
| **Ollama** | Локальные модели через Ollama |
| **Xinference** | Распределённый инференс |
| **Кастомные** | Любой API в формате OpenAI |

## Добавление OpenAI-совместимого провайдера

1. **Настройки → Провайдеры моделей → OpenAI-API-compatible**
2. Заполните:
   - **Имя модели**: например, `my-llama3`
   - **URL API**: например, `http://localhost:8000/v1`
   - **API-ключ**: если требуется
3. **Сохраните** и протестируйте

## Использование Ollama

```bash
# Установите Ollama
curl -fsSL https://ollama.ai/install.sh | sh

# Скачайте модель
ollama pull llama3

# Работает на localhost:11434
```

В Dify:
1. Добавьте Ollama как провайдер
2. URL: `http://host.docker.internal:11434` (если Dify в Docker)
3. Выберите модели из списка

## Дообученные модели

Если вы дообучили модель (на OpenAI или Hugging Face):
1. Разверните её с OpenAI-совместимым API
2. Добавьте как кастомный провайдер в Dify
3. Используйте как встроенную модель

## Балансировка нагрузки

Dify поддерживает несколько ключей для одной модели. Запросы распределяются между ними, что помогает с лимитами.

## Следующий шаг

Давайте создадим продвинутые воркфлоу!
